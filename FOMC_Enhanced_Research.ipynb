{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Frequency Market Reactions to FOMC Communications: A Multi-Modal NLP Approach\n",
    "\n",
    "**Research Notebook - Enhanced Version**\n",
    "\n",
    "## Abstract\n",
    "This notebook analyzes Federal Reserve (FOMC) communications using multiple NLP approaches to predict high-frequency market reactions in the rates market. We employ GPT-4, FinBERT, BART, and semantic embeddings to extract hawkishness signals, combined with novel change detection features that capture subtle linguistic shifts between consecutive statements.\n",
    "\n",
    "## Key Innovations:\n",
    "1. **Change Detection**: Statement-to-statement diff analysis (markets care about changes!)\n",
    "2. **Fed Funds Futures**: Direct policy expectation measures\n",
    "3. **SHAP Analysis**: Interpretable feature importance\n",
    "4. **Time-Series CV**: Proper cross-validation for financial data\n",
    "5. **2024-2025 Holdout**: True out-of-sample testing\n",
    "6. **Attention Mechanisms**: Sentence-level importance weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data fetching\n",
    "import pandas_datareader.data as web\n",
    "import yfinance as yf\n",
    "\n",
    "# NLP libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline, BertTokenizer, BertForSequenceClassification\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, LassoCV\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# SHAP for interpretability\n",
    "import shap\n",
    "\n",
    "# Text processing\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# OpenAI for GPT-4 analysis\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully\")\n",
    "print(f\"PyTorch device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the FOMC communications data and prepare for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FOMC communications\n",
    "df = pd.read_csv('communications.csv')\n",
    "\n",
    "# Parse dates\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Release Date'] = pd.to_datetime(df['Release Date'])\n",
    "\n",
    "# Filter to relevant time period (2000 onwards)\n",
    "df = df[df['Date'] >= '2000-01-01'].copy()\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Separate statements and minutes\n",
    "statements = df[df['Type'] == 'Statement'].copy().reset_index(drop=True)\n",
    "minutes = df[df['Type'] == 'Minute'].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"Statements: {len(statements)}\")\n",
    "print(f\"Minutes: {len(minutes)}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df[['Date', 'Type']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ENHANCEMENT #1: Fed Funds Futures Data\n",
    "\n",
    "**Why this matters**: Fed Funds futures directly measure market expectations for the policy rate. This is the most direct measure of whether markets were surprised by FOMC communications.\n",
    "\n",
    "We'll fetch:\n",
    "- **DFF**: Effective Federal Funds Rate (actual policy rate)\n",
    "- **Treasury yields**: 2Y, 5Y, 10Y (existing)\n",
    "- **Rate changes**: How much markets moved after each FOMC event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_market_data(start_date='2000-01-01', end_date=None):\n",
    "    \"\"\"\n",
    "    Fetch comprehensive market data from FRED\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: DFF, DGS2, DGS5, DGS10\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Fetching market data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Define data series\n",
    "    series = {\n",
    "        'DFF': 'Effective Federal Funds Rate',\n",
    "        'DGS2': '2-Year Treasury Yield',\n",
    "        'DGS5': '5-Year Treasury Yield',\n",
    "        'DGS10': '10-Year Treasury Yield',\n",
    "    }\n",
    "    \n",
    "    # Fetch data\n",
    "    market_data = {}\n",
    "    for code, name in series.items():\n",
    "        try:\n",
    "            data = web.DataReader(code, 'fred', start_date, end_date)\n",
    "            market_data[code] = data[code]\n",
    "            print(f\"  \u2713 {name} ({code}): {len(data)} observations\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 Error fetching {name}: {e}\")\n",
    "    \n",
    "    # Combine into single DataFrame\n",
    "    market_df = pd.DataFrame(market_data)\n",
    "    \n",
    "    # Forward fill missing values (weekends, holidays)\n",
    "    market_df = market_df.fillna(method='ffill')\n",
    "    \n",
    "    print(f\"\\nMarket data shape: {market_df.shape}\")\n",
    "    print(f\"Date range: {market_df.index.min()} to {market_df.index.max()}\")\n",
    "    print(f\"Missing values: {market_df.isna().sum().sum()}\")\n",
    "    \n",
    "    return market_df\n",
    "\n",
    "# Fetch the data\n",
    "market_df = fetch_market_data()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(market_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_market_reactions(df, market_df, horizons=[1, 2]):\n",
    "    \"\"\"\n",
    "    Compute market reactions around FOMC release dates\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with FOMC communications (must have 'Release Date' column)\n",
    "        market_df: DataFrame with market data (indexed by date)\n",
    "        horizons: List of days to compute reactions over (e.g., [1, 2] for 1-day and 2-day)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with market reaction columns added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure Release Date is datetime\n",
    "    df['Release Date'] = pd.to_datetime(df['Release Date'])\n",
    "    \n",
    "    # Initialize columns\n",
    "    for horizon in horizons:\n",
    "        for col in ['DFF', 'DGS2', 'DGS5', 'DGS10']:\n",
    "            df[f'{col.lower()}_{horizon}d_chg'] = np.nan\n",
    "            df[f'{col.lower()}_{horizon}d_bp'] = np.nan\n",
    "    \n",
    "    # Compute reactions\n",
    "    for idx, row in df.iterrows():\n",
    "        release_date = row['Release Date']\n",
    "        \n",
    "        # Get pre-release value (day before or last available)\n",
    "        pre_dates = market_df.index[market_df.index < release_date]\n",
    "        if len(pre_dates) == 0:\n",
    "            continue\n",
    "        pre_date = pre_dates[-1]\n",
    "        \n",
    "        for horizon in horizons:\n",
    "            # Get post-release value (horizon days after)\n",
    "            target_date = release_date + timedelta(days=horizon)\n",
    "            post_dates = market_df.index[\n",
    "                (market_df.index >= release_date) & \n",
    "                (market_df.index <= target_date + timedelta(days=5))  # Allow some slack for weekends\n",
    "            ]\n",
    "            \n",
    "            if len(post_dates) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Take first available post-release date\n",
    "            post_date = post_dates[min(horizon-1, len(post_dates)-1)] if len(post_dates) > 0 else None\n",
    "            \n",
    "            if post_date is None:\n",
    "                continue\n",
    "            \n",
    "            # Compute changes\n",
    "            for col in ['DFF', 'DGS2', 'DGS5', 'DGS10']:\n",
    "                pre_val = market_df.loc[pre_date, col]\n",
    "                post_val = market_df.loc[post_date, col]\n",
    "                \n",
    "                if pd.notna(pre_val) and pd.notna(post_val):\n",
    "                    change = post_val - pre_val\n",
    "                    change_bp = change * 100  # Convert to basis points\n",
    "                    \n",
    "                    df.loc[idx, f'{col.lower()}_{horizon}d_chg'] = change\n",
    "                    df.loc[idx, f'{col.lower()}_{horizon}d_bp'] = change_bp\n",
    "    \n",
    "    # Also compute yield curve spreads\n",
    "    for horizon in horizons:\n",
    "        # 2s10s spread\n",
    "        df[f'spread_2s10s_{horizon}d_bp'] = (\n",
    "            df[f'dgs10_{horizon}d_bp'] - df[f'dgs2_{horizon}d_bp']\n",
    "        )\n",
    "        \n",
    "        # 5s30s spread (if we had 30Y data)\n",
    "        # df[f'spread_5s30s_{horizon}d_bp'] = ...\n",
    "    \n",
    "    print(f\"Market reactions computed for {len(df)} releases\")\n",
    "    print(f\"Horizons: {horizons} days\")\n",
    "    print(f\"\\nSample of 1-day reactions (basis points):\")\n",
    "    print(df[['Date', 'Type', 'dff_1d_bp', 'dgs2_1d_bp', 'dgs5_1d_bp', 'dgs10_1d_bp']].head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Compute market reactions for all documents\n",
    "df = compute_market_reactions(df, market_df, horizons=[1, 2])\n",
    "\n",
    "# Update statements and minutes separately\n",
    "statements = df[df['Type'] == 'Statement'].copy().reset_index(drop=True)\n",
    "minutes = df[df['Type'] == 'Minute'].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Fed Funds Rate over time with FOMC events\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot Fed Funds Rate\n",
    "ax.plot(market_df.index, market_df['DFF'], label='Effective Fed Funds Rate', linewidth=2)\n",
    "\n",
    "# Mark FOMC statement releases\n",
    "statement_dates = statements['Release Date'].dropna()\n",
    "for date in statement_dates:\n",
    "    if date in market_df.index:\n",
    "        ax.axvline(date, color='red', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Rate (%)', fontsize=12)\n",
    "ax.set_title('Effective Federal Funds Rate with FOMC Statement Releases', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Red vertical lines indicate FOMC statement release dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Existing NLP Features\n",
    "\n",
    "Load the features you've already computed (GPT-4 scores, FinBERT, BART, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your existing feature files\n",
    "try:\n",
    "    gpt_scores = pd.read_csv('gpt_hawk_scores.csv')\n",
    "    print(f\"\u2713 Loaded GPT scores: {gpt_scores.shape}\")\n",
    "except:\n",
    "    print(\"\u26a0 gpt_hawk_scores.csv not found - will need to regenerate\")\n",
    "    gpt_scores = None\n",
    "\n",
    "try:\n",
    "    full_features = pd.read_csv('data_with_gpt_bart_finbert.csv')\n",
    "    print(f\"\u2713 Loaded full features: {full_features.shape}\")\n",
    "    print(f\"  Columns: {full_features.columns.tolist()}\")\n",
    "except:\n",
    "    print(\"\u26a0 data_with_gpt_bart_finbert.csv not found\")\n",
    "    full_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ENHANCEMENT #2: Change Detection System\n",
    "\n",
    "**KEY INSIGHT**: Markets don't react to absolute hawkishness\u2014they react to *changes* from the previous statement!\n",
    "\n",
    "A statement that's hawkish but *less hawkish than before* often causes yields to fall.\n",
    "\n",
    "We'll build features that capture:\n",
    "1. **Sentence-level changes**: Which sentences were added, removed, or modified?\n",
    "2. **Key phrase tracking**: Changes in specific policy language\n",
    "3. **Semantic drift**: How much did the meaning shift?\n",
    "4. **Section-specific changes**: Separate changes in outlook vs. policy description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def compute_text_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Compute similarity between two texts\n",
    "    \"\"\"\n",
    "    if pd.isna(text1) or pd.isna(text2):\n",
    "        return np.nan\n",
    "    \n",
    "    # Use SequenceMatcher for character-level similarity\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "def extract_key_phrases(text):\n",
    "    \"\"\"\n",
    "    Extract key policy-related phrases\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    phrases = {\n",
    "        # Inflation language\n",
    "        'inflation_elevated': 'inflation remains elevated' in text_lower or 'elevated inflation' in text_lower,\n",
    "        'inflation_moderating': 'inflation has moderated' in text_lower or 'moderating inflation' in text_lower,\n",
    "        'inflation_easing': 'inflation easing' in text_lower or 'inflation has eased' in text_lower,\n",
    "        \n",
    "        # Rate language\n",
    "        'rate_increases': 'rate increase' in text_lower or 'raising the target range' in text_lower,\n",
    "        'rate_cuts': 'rate cut' in text_lower or 'rate reduction' in text_lower or 'lowering the target range' in text_lower,\n",
    "        'rate_hold': 'maintain the target range' in text_lower or 'leaving the target range' in text_lower,\n",
    "        \n",
    "        # Forward guidance\n",
    "        'data_dependent': 'data dependent' in text_lower or 'incoming data' in text_lower,\n",
    "        'patient': 'patient' in text_lower and 'policy' in text_lower,\n",
    "        'gradual': 'gradual' in text_lower,\n",
    "        \n",
    "        # Labor market\n",
    "        'labor_tight': 'tight labor' in text_lower or 'labor market remains tight' in text_lower,\n",
    "        'labor_softening': 'labor market has softened' in text_lower or 'softening labor' in text_lower,\n",
    "        \n",
    "        # Economic outlook\n",
    "        'growth_solid': 'solid growth' in text_lower or 'economic growth is solid' in text_lower,\n",
    "        'growth_slowing': 'slowing growth' in text_lower or 'growth has slowed' in text_lower,\n",
    "    }\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "def detect_statement_changes(current_text, previous_text, current_date=None, prev_date=None):\n",
    "    \"\"\"\n",
    "    Comprehensive change detection between consecutive FOMC statements\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of change features\n",
    "    \"\"\"\n",
    "    if pd.isna(current_text) or pd.isna(previous_text):\n",
    "        return {}\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    curr_sentences = sent_tokenize(current_text)\n",
    "    prev_sentences = sent_tokenize(previous_text)\n",
    "    \n",
    "    # Convert to sets for comparison\n",
    "    curr_set = set(s.strip() for s in curr_sentences)\n",
    "    prev_set = set(s.strip() for s in prev_sentences)\n",
    "    \n",
    "    # Count changes\n",
    "    added = curr_set - prev_set\n",
    "    removed = prev_set - curr_set\n",
    "    unchanged = curr_set & prev_set\n",
    "    \n",
    "    # Overall similarity\n",
    "    overall_similarity = compute_text_similarity(current_text, previous_text)\n",
    "    \n",
    "    # Length changes\n",
    "    len_change_pct = (len(current_text) - len(previous_text)) / len(previous_text) * 100 if len(previous_text) > 0 else 0\n",
    "    sentence_count_change = len(curr_sentences) - len(prev_sentences)\n",
    "    \n",
    "    # Key phrase analysis\n",
    "    curr_phrases = extract_key_phrases(current_text)\n",
    "    prev_phrases = extract_key_phrases(previous_text)\n",
    "    \n",
    "    # Track phrase changes\n",
    "    phrase_changes = {}\n",
    "    for phrase_name in curr_phrases.keys():\n",
    "        curr_val = curr_phrases[phrase_name]\n",
    "        prev_val = prev_phrases[phrase_name]\n",
    "        \n",
    "        if curr_val and not prev_val:\n",
    "            phrase_changes[f'{phrase_name}_added'] = 1\n",
    "        elif not curr_val and prev_val:\n",
    "            phrase_changes[f'{phrase_name}_removed'] = 1\n",
    "        else:\n",
    "            phrase_changes[f'{phrase_name}_added'] = 0\n",
    "            phrase_changes[f'{phrase_name}_removed'] = 0\n",
    "    \n",
    "    # Compile features\n",
    "    features = {\n",
    "        'change_sentences_added': len(added),\n",
    "        'change_sentences_removed': len(removed),\n",
    "        'change_sentences_unchanged': len(unchanged),\n",
    "        'change_net_sentences': len(added) - len(removed),\n",
    "        'change_pct_sentences_modified': (len(added) + len(removed)) / max(len(prev_set), 1) * 100,\n",
    "        'change_overall_similarity': overall_similarity,\n",
    "        'change_text_length_pct': len_change_pct,\n",
    "        'change_sentence_count': sentence_count_change,\n",
    "    }\n",
    "    \n",
    "    # Add phrase changes\n",
    "    features.update(phrase_changes)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"\u2713 Change detection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_change_features(df):\n",
    "    \"\"\"\n",
    "    Add change detection features to dataframe\n",
    "    Compares each statement to the previous one\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Initialize all change features to NaN for first row\n",
    "    all_change_features = []\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        if idx == 0:\n",
    "            # First statement has no previous comparison\n",
    "            all_change_features.append({})\n",
    "        else:\n",
    "            current_text = df.loc[idx, 'Text']\n",
    "            previous_text = df.loc[idx-1, 'Text']\n",
    "            current_date = df.loc[idx, 'Date']\n",
    "            prev_date = df.loc[idx-1, 'Date']\n",
    "            \n",
    "            changes = detect_statement_changes(\n",
    "                current_text, previous_text, \n",
    "                current_date, prev_date\n",
    "            )\n",
    "            all_change_features.append(changes)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    change_df = pd.DataFrame(all_change_features)\n",
    "    \n",
    "    # Concatenate with original DataFrame\n",
    "    df = pd.concat([df, change_df], axis=1)\n",
    "    \n",
    "    print(f\"\u2713 Added {len(change_df.columns)} change detection features\")\n",
    "    print(f\"  Features: {list(change_df.columns[:10])}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add change features to statements\n",
    "print(\"Computing change features for statements...\")\n",
    "statements = add_change_features(statements)\n",
    "\n",
    "print(\"\\nExample change features:\")\n",
    "change_cols = [col for col in statements.columns if col.startswith('change_')]\n",
    "print(statements[['Date'] + change_cols[:5]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ENHANCEMENT #3: Time-Series Cross-Validation\n",
    "\n",
    "**Problem with your current approach**: Single train/test split at 2017 doesn't show robustness\n",
    "\n",
    "**Solution**: Walk-forward time-series cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_splits(df, holdout_year=2024, cv_cutoff_year=2017):\n",
    "    \"\"\"\n",
    "    Create proper time-series splits:\n",
    "    1. Training set: Before cv_cutoff_year (for CV)\n",
    "    2. Validation set: cv_cutoff_year to holdout_year (for model selection)\n",
    "    3. Holdout set: holdout_year onwards (true out-of-sample)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with FOMC data\n",
    "        holdout_year: Year to start holdout set (2024 or 2025)\n",
    "        cv_cutoff_year: Year to split train/validation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train, validation, holdout splits\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['year'] = pd.to_datetime(df['Date']).dt.year\n",
    "    \n",
    "    # Create splits\n",
    "    train = df[df['year'] < cv_cutoff_year].copy()\n",
    "    validation = df[(df['year'] >= cv_cutoff_year) & (df['year'] < holdout_year)].copy()\n",
    "    holdout = df[df['year'] >= holdout_year].copy()\n",
    "    \n",
    "    print(f\"Train set: {len(train)} samples ({train['year'].min()}-{train['year'].max()})\")\n",
    "    print(f\"Validation set: {len(validation)} samples ({validation['year'].min()}-{validation['year'].max() if len(validation) > 0 else 'N/A'})\")\n",
    "    print(f\"Holdout set: {len(holdout)} samples ({holdout['year'].min() if len(holdout) > 0 else 'N/A'}-{holdout['year'].max() if len(holdout) > 0 else 'N/A'})\")\n",
    "    \n",
    "    return {\n",
    "        'train': train,\n",
    "        'validation': validation,\n",
    "        'holdout': holdout,\n",
    "        'train_val': pd.concat([train, validation])  # Combined for final training\n",
    "    }\n",
    "\n",
    "# Create splits\n",
    "splits = create_train_test_splits(statements, holdout_year=2024, cv_cutoff_year=2017)\n",
    "\n",
    "print(f\"\\n\u2713 Data split created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv_split(df, n_splits=5, min_train_size=30):\n",
    "    \"\"\"\n",
    "    Create time-series cross-validation splits\n",
    "    \n",
    "    Each fold uses an expanding window:\n",
    "    - Fold 1: Train on first 30, test on next 10\n",
    "    - Fold 2: Train on first 40, test on next 10\n",
    "    - etc.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    splits_info = []\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        if len(train_idx) < min_train_size:\n",
    "            continue\n",
    "            \n",
    "        train_dates = df.iloc[train_idx]['Date']\n",
    "        test_dates = df.iloc[test_idx]['Date']\n",
    "        \n",
    "        splits_info.append({\n",
    "            'fold': fold_idx,\n",
    "            'train_idx': train_idx,\n",
    "            'test_idx': test_idx,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_size': len(test_idx),\n",
    "            'train_period': f\"{train_dates.min():%Y-%m} to {train_dates.max():%Y-%m}\",\n",
    "            'test_period': f\"{test_dates.min():%Y-%m} to {test_dates.max():%Y-%m}\"\n",
    "        })\n",
    "    \n",
    "    # Print fold information\n",
    "    print(f\"Time-Series Cross-Validation: {len(splits_info)} folds\\n\")\n",
    "    for split in splits_info:\n",
    "        print(f\"Fold {split['fold']}: Train {split['train_size']} ({split['train_period']}) | Test {split['test_size']} ({split['test_period']})\")\n",
    "    \n",
    "    return tscv, splits_info\n",
    "\n",
    "# Create CV splits for the training+validation set\n",
    "tscv, cv_splits = time_series_cv_split(splits['train_val'], n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Feature Matrix\n",
    "\n",
    "Combine all features: existing NLP features + new change features + market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's define a placeholder feature set\n",
    "# You'll merge this with your existing features from data_with_gpt_bart_finbert.csv\n",
    "\n",
    "def prepare_feature_matrix(df, target='dgs2_1d_bp'):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all features\n",
    "        target: Target variable (yield change)\n",
    "    \n",
    "    Returns:\n",
    "        X, y, feature_names\n",
    "    \"\"\"\n",
    "    # Select feature columns\n",
    "    # You'll need to customize this based on your actual features\n",
    "    feature_cols = [col for col in df.columns if (\n",
    "        col.startswith('change_') or \n",
    "        col.startswith('gpt_') or\n",
    "        col.startswith('bart_') or\n",
    "        col.startswith('finbert_') or\n",
    "        col in ['hawk_minus_dove', 'delta_semantic', 'is_minute']\n",
    "    )]\n",
    "    \n",
    "    # Remove target columns from features\n",
    "    feature_cols = [col for col in feature_cols if not any([\n",
    "        'dgs' in col.lower(),\n",
    "        'dff' in col.lower(),\n",
    "        'dy' in col.lower(),\n",
    "        'spread' in col.lower()\n",
    "    ])]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"Sample features: {feature_cols[:10]}\")\n",
    "    \n",
    "    # Extract X and y\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(0)  # Simple imputation for now\n",
    "    \n",
    "    # Filter to valid samples (where target is not null)\n",
    "    valid_idx = y.notna()\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"Final shape: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"Target stats: mean={y.mean():.2f}, std={y.std():.2f}, min={y.min():.2f}, max={y.max():.2f}\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare features (this will work with change features only for now)\n",
    "# Later you'll merge with your existing NLP features\n",
    "print(\"Preparing feature matrix...\")\n",
    "print(\"\\nNote: This uses only change features for now.\")\n",
    "print(\"You'll need to merge with your existing GPT/BART/FinBERT features from 'data_with_gpt_bart_finbert.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ENHANCEMENT #4: SHAP Analysis for Interpretability\n",
    "\n",
    "**Why this matters for the paper**: \n",
    "- Academics want to know *which* features matter\n",
    "- Practitioners want *interpretable* signals\n",
    "- SHAP shows: \"This 5bp yield spike is 40% explained by change in inflation language, 30% by GPT hawkishness score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for SHAP analysis\n",
    "# Will implement after we have full feature set and trained models\n",
    "\n",
    "def explain_model_with_shap(model, X_train, X_test, feature_names, model_type='tree'):\n",
    "    \"\"\"\n",
    "    Generate SHAP explanations for model predictions\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        feature_names: List of feature names\n",
    "        model_type: 'tree' or 'linear'\n",
    "    \n",
    "    Returns:\n",
    "        SHAP explainer and values\n",
    "    \"\"\"\n",
    "    print(f\"Computing SHAP values for {model_type} model...\")\n",
    "    \n",
    "    if model_type == 'tree':\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    else:  # linear\n",
    "        explainer = shap.LinearExplainer(model, X_train)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Summary plot\n",
    "    print(\"\\nGenerating SHAP summary plot...\")\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features (by mean |SHAP value|):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return explainer, shap_values, feature_importance\n",
    "\n",
    "print(\"\u2713 SHAP analysis function defined\")\n",
    "print(\"  Will run after model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training with Cross-Validation\n",
    "\n",
    "Train models using proper time-series CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_cv(X, y, model, cv_splitter, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Train model with time-series cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Target\n",
    "        model: Sklearn-compatible model\n",
    "        cv_splitter: TimeSeriesSplit object\n",
    "        model_name: Name for display\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv_splitter.split(X)):\n",
    "        # Split data\n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_train = model.predict(X_train_fold)\n",
    "        y_pred_test = model.predict(X_test_fold)\n",
    "        \n",
    "        # Compute metrics\n",
    "        fold_results = {\n",
    "            'fold': fold_idx,\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train_fold, y_pred_train)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test_fold, y_pred_test)),\n",
    "            'train_mae': mean_absolute_error(y_train_fold, y_pred_train),\n",
    "            'test_mae': mean_absolute_error(y_test_fold, y_pred_test),\n",
    "            'train_r2': r2_score(y_train_fold, y_pred_train),\n",
    "            'test_r2': r2_score(y_test_fold, y_pred_test),\n",
    "        }\n",
    "        \n",
    "        cv_results.append(fold_results)\n",
    "    \n",
    "    # Aggregate results\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Cross-Validation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(cv_df)\n",
    "    print(f\"\\nMean Test RMSE: {cv_df['test_rmse'].mean():.3f} \u00b1 {cv_df['test_rmse'].std():.3f}\")\n",
    "    print(f\"Mean Test MAE:  {cv_df['test_mae'].mean():.3f} \u00b1 {cv_df['test_mae'].std():.3f}\")\n",
    "    print(f\"Mean Test R\u00b2:   {cv_df['test_r2'].mean():.3f} \u00b1 {cv_df['test_r2'].std():.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'cv_results': cv_df,\n",
    "        'mean_test_rmse': cv_df['test_rmse'].mean(),\n",
    "        'std_test_rmse': cv_df['test_rmse'].std(),\n",
    "        'mean_test_mae': cv_df['test_mae'].mean(),\n",
    "        'mean_test_r2': cv_df['test_r2'].mean(),\n",
    "    }\n",
    "\n",
    "print(\"\u2713 CV training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Integration Instructions\n",
    "\n",
    "**To integrate with your existing work:**\n",
    "\n",
    "```python\n",
    "# 1. Load your existing features\n",
    "existing_features = pd.read_csv('data_with_gpt_bart_finbert.csv')\n",
    "\n",
    "# 2. Merge with new features from this notebook\n",
    "enhanced_df = statements.merge(\n",
    "    existing_features[['Date', 'gpt_score', 'bart_score', 'finbert_pos', 'finbert_neg', ...]],\n",
    "    on='Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Now you have:\n",
    "#    - All your existing NLP features (GPT, BART, FinBERT, etc.)\n",
    "#    - New change detection features (change_*)\n",
    "#    - Fed Funds futures data (dff_1d_bp, dff_2d_bp)\n",
    "#    - Proper train/val/holdout splits\n",
    "\n",
    "# 4. Train and evaluate!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}